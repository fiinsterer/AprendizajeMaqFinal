\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm,float,bbm}
\usepackage{graphicx}

\author{Jorge Carlos Urteaga \and Víctor R. Martinez}
\title{Falta titulo}
\date{\today}
\decimalpoint

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle

<<echo=FALSE>>=
library(ggplot2)
library(glmnet)
library(e1071)
library(plyr)
library(reshape2)
library(nnet)
set.seed(107346)

alacranes <- read.csv("csvlist1.csv")
alacranes[which(alacranes$tipo == 1111), c("tipo")] <- 0
alacranes[which(alacranes$tipo == 2222), c("tipo")] <- 1
alacranes$tipo <- as.factor(alacranes$tipo)
#Quitamos la columna ID
alacranes <- alacranes[,!names(alacranes) %in% c("ID")]

alacranes.scaled <- data.frame(scale(alacranes[,-141]),tipo = alacranes$tipo)

obs <- sample(seq(1, dim(alacranes.scaled)[1]),dim(alacranes.scaled)[1], replace=F)
max <- length(obs)/5
kfolds <- split(obs, ceiling(seq_along(obs)/max))
@



\section{Introducción}
???

\section{Datos}
???

\section{Desarrollo}
\subsection{Procesamiento inicial}
En primer lugar los datos fueron escalados y centrados a media $0$ y varianza $1$ para eliminar el ruido producto de utilizar diferentes unidades entre las variables. Además, si no hacíamos este paso, el procedimiento de reducción de dimensión fallaba por razones aún desconocidas.

<<>>=
#Escalamos las variables a mu = 0, sigma = 1
alacranes.scaled <- data.frame(scale(alacranes[,-141]),
                               tipo = alacranes$tipo)
@

A continuación, reducimos la dimensionalidad del problema utilizando un Análisis de Componentes Principales. Encontramos que con sólo 34 factores eramos capaces de capturar el 95.814\% de la varianza de los datos.

<<>>=
pca <- princomp(~., data=alacranes.scaled[, -141],
                cor = TRUE, na.action=na.exclude)
datos.red <- data.frame(pca$scores[,1:34],
                        tipo = alacranes.scaled$tipo)
@

A partir de los componentes más importantes, podemos representar nuestro problema en dos dimensiones para una mejor interpretación de los datos y una primera aproximación al procedimiento que tendremos que utilizar. Como podemos ver en la siguiente figura, los alacranes no venenosos se encuentran agrupados en el centro de los datos rodeados por alacranes venenosos. Es claro que un modelo lineal sería incapaz de separar de manera adecuada los datos, necesitamos un modelo capaz de generar áreas de clasificación circulares.
<<fig=TRUE>>=
ggplot(aes(x = Comp.1, y = Comp.2, color = tipo),
       data = datos.red) + geom_point()
@

\subsection{Métodos}
\subsubsection{Red neuronal}
Nuestra primera aproximación al problema fue mediante el uso de una red neuronal. Se entrenó una red neuronal con 12 neuronas en la capa escondida (número determinado de manera empírica) con decaemiento ($10^{-9}$) en un máximo de 1000 iteraciones. Para medir el comportamiento de la red, se utilizó un procedimiento de validación cruzada a 5 iteraciones.
<<>>=
################################################################
#                           ANN                                #
################################################################
ann.cross.validation <- sapply(seq(1,5), function(i){
  train <- datos.red[unlist(kfolds[c(-i)]),]
  test <- datos.red[kfolds[[i]],]
  ann <- nnet(tipo ~ ., data = train, size = 12,
              decay = 1E-9, maxit = 1000)
  preds <- predict(ann, newdata = test)
  mean(preds == test$tipo)
})
mean(ann.cross.validation)
@
En todas las iteraciones de validación cruzada la red convergió. El resultado final fue decepcionante, ya que la red sólo fue capaz de predecir $72.465\%$ de los casos. Como se verá en la siguiente sección, la máquina de soporte vectorial alcanzó mejores resultados en comparación.

\subsection{Máquina de soporte vectorial}
El segundo método estudiado fueron las máquinas de soporte vectorial. Debido a la forma estudiada de los datos se optó por utilizar un \emph{kernel} de tipo radial, con gamma igual a $2$ (determinado empíricamente). De nueva cuenta recurrimos a la validación cruzada de 5 iteraciones para determinar el mejor parámetro de castigo $C$. El espacio explorado fueron las potencias de 10 desde $-3$ hasta $5$. La siguiente figura muestra el promedio de la certeza del estimado junto con sus barras de error estándar con parámetro $C$ en escala logarítmica. Podemos ver que para $C$ pequeñas, el desempeño del estimador es estadísticamente más bajo que para $C > 100$. Además, la certeza del estimador es equivalente para $C \in \{100, 1000, 10000\}$.

<<fig=TRUE>>=
cross.validation <- sapply(c(0.001, 0.01, 0.1, 1,
                             10, 100, 1000, 10000),function(C){
  medias.C <- sapply(seq(1, 5), function(i){
    train <- datos.red[unlist(kfolds[c(-i)]),]
    test <- datos.red[kfolds[[i]],]
    svm.c <- svm(tipo ~ ., data = train, 
                 kernel = "radial", gamma = 2,
                 cost = C)
    preds <- predict(svm.c, newdata = test)
    mean(preds == test$tipo)
  })
})
cross.validation <- t(cross.validation)
cv.summarize <- data.frame(C = c(0.001, 0.01, 0.1, 1,
                                 10, 100, 1000, 10000), 
                           mean = rowSums(cross.validation)/5, 
                           sd = apply(cross.validation, 1, sd))
ggplot(aes(x = log(C), y = mean),
       data = cv.summarize) + geom_errorbar(aes(ymin = mean - sd,
                                                ymax = mean + sd, 
                                                width=.1)) + geom_point()
@

Para $C > 100$, este clasificador obtuvo una tasa media de clasificación de $85.612\%$ con una desviación estándar de $0.0499$. 

De nueva cuenta, podemos graficar el comportamiento del estimador en dos dimensiones (a partir de los principales factores encontrados). A continuación se muestra como es que el SVM de \emph{kernel} radial con parámetro de castigo $C = 100$ clasifica los puntos del espacio $\mathbb{R}^2$. 

<<fig =TRUE>>=
svm.opt <- svm(tipo ~ Comp.1 + Comp.2, data = datos.red,
               kernel = "radial", gamma = 2, cost = 100)
dat.x <- expand.grid(Comp.1 = seq(-30,30,0.5),
                     Comp.2 = seq(-20,20,0.5))
dat.x$preds.1 <- predict(svm.opt, newdata = dat.x)
ggplot(dat.x, aes(x = Comp.1, y = Comp.2, 
                  colour = preds.1)) + 
  geom_point(size = 1) +
  geom_point(aes(x = Comp.1, y = Comp.2,
                 color = tipo), data = datos.red, size = 3)
@
\subsubsection{Sensibilidad, Precisión y score F1}
Como se vió en clase, podemos estudiar a nuestro clasificador en términos de las proporciones de casos que caen en cada celda, que dependen del desempeño del clasificador en cuanto a casos positivos y negativos. Para nuestro problema, definimos un caso positivo como el caso donde el alacrán es venenoso, y uno negativo como el caso donde no lo es.

Definimos \emph{sensibilidad} como la tasa de positivos verdaderos y representa la medida de que tan bien nuestro clasificador detecta los casos positivos. Tenemos
<<>>=
tp <- which(datos.red$tipo == 1) == which(predict(svm.opt, newdata = datos.red) == 1)
sens <- 
@

\subsubsection{Mejorando el modelo: Sesgo y Varianza}
Ahora nos interesa saber si es posible conseguir mejores tasas de clasificación con un clasificador SVM y la manera de hacerlo. Para esto, estudiamos el error de entrenamiento y el error de prueba a distintos tamaños de muestras. Para cada tamaño $n$ era importante asegurarnos que la muestra de entrenamiento tuviera ejemplos de ambas categorías, pues de no ser asi, el modelo SVM no sería capaz de obtener los hiper-planos separadores (pues no hay nada que separar). Por esta razón, utilizamos un muestreo estratificado como se muestra a continuación
<<>>=
# Ocupamos statrified sampling para asegurar que tenemos
# al menos un elemento de cada clase en la muestra
# evitando el error de SVM vacio
##
idx.tipo1 <- sample(which(datos.red$tipo == 1), .8 * 174)
idx.tipo0 <- sample(which(datos.red$tipo == 0), .8 * 62)
train <- rbind(datos.red[idx.tipo1,], datos.red[idx.tipo0,])
test <- rbind(datos.red[-idx.tipo0,],datos.red[-idx.tipo1,])
#Shuffle data
train <- train[sample(dim(train)[1]),]
test <- test[sample(dim(test)[1]),]
@
Al revolver los datos (último paso) nos aseguramos de que, aun en el peor de los casos, una muestra de tamaño 49 tendrá ambas categorías con probabilidad muy alta (cercana a 1). En nuestros experimentos encontramos que el tamaño de la muestra podia ser bastante más bajo. 

Generamos las curvas comparativas de error de entrenamiento y error de prueba para ver si el error que tenemos es producto de sesgo (y no vale la pena agrandar el tamaño de muestra) o varianza (y una muestra más grande podria mejorar nuestro modelo).
<<fig=TRUE>>=
errs <- sapply(seq(20, dim(train)[1]), function(i){
  svm.opt <- svm(tipo ~ ., data = train[1:i,],
                 kernel = "radial", gamma = 2, cost = 100)
  #Err train
  preds.train <- predict(svm.opt, newdata = train[1:i,])
  err.train <- mean(preds.train != train[1:i,]$tipo)
  #Err test
  preds.test <- predict(svm.opt, newdata = test)
  err.test <- mean(preds.test != test$tipo)
  list(i, err.train, err.test)
})
errs <- t(errs)
errs.df.m<- melt(data.frame(i = unlist(errs[,1]),
                            err.train = unlist(errs[,2]),
                            err.test = unlist(errs[,3]))
                 , id=c("i"))
ggplot(aes(x = i, y = value, colour=variable),
       data = errs.df.m) + geom_line()
@

En esta gráfica podemos observar que las curvas de entrenamiento y validación se encuentran separadas y existe una tendencia de la curva de validación a la baja. Esto sugiere que nuestro error es producto de varianza y que necesitamos obtener una muestra de mayor tamaño para reducir el error.





\section{Resultados}
\section{Conclusiones}

\end{document}